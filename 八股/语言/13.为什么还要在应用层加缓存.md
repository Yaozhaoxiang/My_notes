# 既然操作系统层已经提供了page cache的功能，为什么还要在应用层加缓存？

操作系统的磁盘操作已经提供了page cache，比如Linux会把所有未使用的内存用作page cache，那么我们为什么还需要加那么多层缓存？DB有自己的缓存，为什么我们还需要像memcached这样的应用层缓存？


原因：
持久性是写数据的需求，读数据的时候，因为距离应用代码更近，应用层缓存一般比系统层缓存要更快，例如读文件，每次读的数据都很短，如果没有应用层缓存，就需要频繁的系统调用，这对 cpu 是极大的浪费。另一种方案是 mmap，将 page cache 暴露给应用代码，在很多时候是一个更好的选择。
如果数据在内存中的结构布局和在文件中完全相同，mmap 则近乎完美。


1. 开发层面，mmap file没有完备的stage-commit API。mmap会自动把dirty page写回，你可以强制写回，但你不能要求它不写回。这对一致性控制是减分项——事务还没有commit时，中间结果可能就被持久化了。这让事务的实现更复杂了。应用自己管理缓冲区，就没有这些烦恼。

2. 性能层面，mmap file也有性能局限。
并发：内核中page cache能提供的并发度并不一定能满足应用所需的吞吐量，内核数据结构如果成为瓶颈，开发者将束手无策。

吞吐量：当文件吞吐量过大时，mmap file的会频繁换入换出，换出page cache时，需要修改相关的页表和TLB。页表还好，但是TLB每个核都有一个份，这意味着换出page cache时需要同步清理全核TLB——一次全核中断，对吞吐量和时延的影响都很大。如果bypass掉OS page cache，自己管理一个恒定大小的buffer pool，这时候OS页表和TLB是不受影响的。这篇文章的实验主要在验证这一点。

预取，写回，换入换出等具体的策略，只能用OS提供的，虽然也有几种选择（比如MADV_RANDOM MADV_SEQUENTIAL），但上限肯定远远比不上应用层按需定制的。


总结来说，就是在高并发、高磁盘读写量场景下，OS现有的page cache设计是有不足的。OS page cache的通用API也不能提供对磁盘IO的精细控制。这种使用场景的应用，应当自己实现文件缓存。


> 但对没有这种场景的应用（比如题主提到的kafka），OS page cache大概只有好处：

开发层面，不用向应用层缓存一样记录内存block和文件block的映射关系，不用自己实现脏block写回等。

性能层面，不仅可以减少一次内核态到用户态的拷贝，还可以用减少系统调用的使用（直接操作内存即可，不需要使用大量的read()/write()）。

## 持久化

将文件作为数据的持久存储，持久是个核心问题，它分这几个层面：

1. 数据存入用户态缓存，例如 stdio 中 FILE buffer
2. 数据存入操作系统 page cache，例如我们调用了 write
3. 数据存入硬件设备，例如 fsync/fdatasync
4. 数据存入硬件设备中的持久存储介质。很多硬件设备都有缓存，写入缓存就返回成功可以达到最高性能，但是有数据丢失的风险，于是很多企业级存储设备中就自带电池，既保证性能，又保证持久性（在断电之后仍可以将缓存中的数据写入持久存储介质）

持久性是写数据的需求，读数据的时候，因为距离应用代码更近，应用层缓存一般比系统层缓存要更快，例如读文件，每次读的数据都很短，如果没有应用层缓存，就需要频繁的系统调用，这对 cpu 是极大的浪费。另一种方案是 mmap，将 page cache 暴露给应用代码，在很多时候是一个更好的选择。






