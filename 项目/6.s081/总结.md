1. 添加系统调用：sys_sysinfo, symlink(软连接)，sys_mmap? sys_unmmap?; (lab2，lab9)
2. 修改了创建进程和分配内存的内核代码，为xv6的内核页表添加用户页表的页表项，因此在内核态下可以直接访问用户空间虚拟地址对应的物理地址。  实现用户空间和内核之间共享只读区域中的数据 (lab3)
3. 添加抢占式的进程切换，通过时钟中断陷入内核态周期性地执行一个alarm进程；（lab4）
4. 添加了迟分配和fork的COW(写时复制)机制来节省物理内存，当触发页错误时才分配物理内存;（lab5）
5. 设计内存分配器，使得不同 CPU 上的分配和释放可以并行进行，为每个 CPU 维护一个可用内存链表；(lab8)
6. 扩展xv6的ext文件系统，在inode中添加多级索引，增加通过单个inode可以访问的数量块的数量，从而增加单个文件容量；(lab9)


exec抽象内存，每次运行都会传入文件名
file抽象了内存，应用程序不会直接读写再计算机上的磁盘本身，通过file交互

satp（Supervisor Address Translation and Protection）寄存器用于保存当前虚拟地址到物理地址的转换模式和相关信息，也就是最高一级的页表目录。物理地址。

操作系统必须满足三个要求：多路复用、隔离和交互

## lab 2 
1. 添加一个系统调用sysinfo，它收集有关正在运行的系统的信息，空闲内存的字节数，
nproc字段应该设置为状态不是UNUSED的进程数


## lab 3
1. Speed up system calls  加快系统调用：用户空间和内核之间共享只读区域中的数据来加速某些系统调用
2. vmprint 打印页表： 接受一个pagetable_t参数，并以下面描述的格式打印该页表
3. sys_pgaccess() 检测访问了哪些页面：返回那个页已经被访问了


#### 1. 虚拟地址
每一个进程都维护自己的一个page_table，这个就像 缓冲池里的page_table一样。
每一个进程都有自己的虚拟空间，也就相当于 pages，
而page_table就是虚拟地址于physical的映射，就像 frame_id与page_id的映射；

那么虚拟地址是什么呢？
为了实现内存隔离，防止一个进程出现错误，而修改另一个进程的地址空间。
每个进程都有自己的虚拟地址地址空间，从0到2^64；每个进程都维护一个page_table,
它的作用就是实现虚拟地址与物理地址的映射。
这就很像缓冲池的实现，缓冲池维护一定数量的page，这些page用一个数组保存在缓冲池中
，每个page都有自己唯一的page_id。假如上层需要一个page,通过这个page_id来拿取
，在缓冲池中用数组保存，所以我们需要一个映射，就是page_id与索引的映射，这个索引
也成为帧，而这个映射就是page_table;

怎么通过虚拟地址找到物理地址呢？
一个进程维护一个 trapframe，这里面保存寄存器的信息，而satp保存一个 kernel page table，
而这个页表就是第一层目录，然后通过 虚拟地址的前9位确定下一层目录的物理地址，一次进行即可。

#### 2. 内核也维护一个页表
内核页维护一个虚拟地址到物理地址的映射，而且映射是一一对应的，这样的好处：
当xv6开始第一个进程的时候，会配置信息，并创建内核表，创建完内核表后，需要跳转。因为目前还处于物理地址状态，需要跳转到虚拟地址。而指令进行到下一步地址是+4，也就是说下一次的执行指令将会是在虚拟地址上完成的，会进行查表一系列操作，但是由于虚拟地址核物理地址对应映射的，所以不会产生错误。

## lab 4
1. Backtrace 实现反向跟踪：跟踪函数上方的堆栈上的函数调用列表
2. Alarm： 添加抢占式的进程切换，通过时钟中断陷入内核态周期性地执行一个alarm进程；


CPU 通常包含用户寄存器和控制寄存器这两种类型的寄存器：
pc
mode
- `stvec`：内核在此处写入中断处理程序的地址；RISC-V 跳转到 `stvec` 中的地址来处理中断。
- `sepc`：当中断发生时，RISC-V 将**程序计数器保存在这里**（因为随后程序计数器会被 `stvec` 中的值覆盖）。`sret`（从中断返回）指令将 `sepc` 复制到程序计数器。内核可以通过写入 `sepc` 来控制 `sret` 返回的位置。
- `scause`：RISC-V 在这里放置一个数字，描述**中断的原因**。
- `sscratch`：内核在此处放置一个值，该值在中断处理程序的非常早期阶段很有用。
- `sstatus`：`sstatus` 中的 SIE 位控制是否启用设备中断。如果内核清除 SIE，RISC-V 将推迟设备中断直到内核设置 SIE。SPP 位指示中断来自用户模式还是超级用户模式，并控制 `sret` 返回到哪个模式。
- `satp`：用于控制虚拟地址到物理地址的转换。

supervisor:
+ R/W : satp, stvec, sepc, sscratch
#### 怎么从用户态切换到内核态呢？
在用户态中首先执行ecall，这个ecall不会切换page_table，此时还是用户态页表，这个时候跳转到 trampoline中，在这里面我需要保存当前用户态的状态，及32个寄存器和控制寄存器的值到 TRAPFRAME 中。在创建进程的时候，我们会给每个进程分配一个页表，并且在页表中添加 trampoline 和 TRAPFRAME的映射与内核态页表，没有用户位，即用户状态下不能访问，并且这个trampoline是可执行的。所以当跳转到 trampoline中，将会保存寄存器到TRAPFRAME中，在这个过程中 会用到 sscratch寄存器，因为我们要保存所有的寄存器状态，所以需要它先替换处 a0，然后使用a0寄存器保存其他寄存器状态，最后保存a0；

这个过程是通过 stevc这个寄存器完成的，内核在此处写入中断处理程序的地址，当有中断时，则跳转到这里，即trampoline；在ecall调用后就通过这个寄存器找到 trampoline；

ecall完成了三件事：1. 将模式从用户态改为supervisor mode；2. 将程序计数器的值保存在sepc中；3.跳转到stvec寄存器指向的指令。

在内核中创建一个进程并返回用户态时，调用ret，在这个过程中将设置寄存器的值和其他控制器的值；并且把内核的一些信息保存到 TRAPFRAME中：
kernel_satp，保存内核态的地址空间标识符（ASID）和页表根节点的物理地址
kernel_sp，保存内核栈指针
kernel_hartid，cpu核的编号，内核可以通过这个值确定某个cpu核上运行哪些进程
epc，保存引起异常的指令地址
kernel_trap；保存内核处理陷阱的入口地址。

此时只是保存了寄存器，还没有进入内核，想要进入内核我们需要内核的一些信息，这些信息保存在TRAPFRAME中，从中加载即可；

返回操作就是书上的，很详细。


#### 堆栈
每次函数调用，函数都会为自己创建一个 stack frame，函数通过移动栈帧来完成stack frame的空间分配。函数栈帧包括返回地址，上一个栈帧，局部变量，寄存器等。如果参数寄存器用完了，二外的参数就会出现在栈上。返回地址在第一位，上一个栈帧在第二个；
栈帧有两个重要的寄存器：
 + sp它指向stack的地步并代表当前stack frame的位置
 + fp指向当前stack frame的顶部

## lab 5
1. 实现 fork 延迟复制机制，在进程 fork 后，不立刻复制内存页，而是将虚拟地址指向与父进程相同的物理地址。在父子任意一方尝试对内存页进行修改时，才对内存页进行复制。 物理内存页必须保证在所有引用都消失后才能被释放，这里需要有引用计数机制。

当发生页面错误的时候，重新分配需要两个信息：
 + 1. va ：虚拟地址保存在 stval寄存器中
 + 2. page fault的类型： r,w,instruction，保存在scause寄存器中
 + 3. sepc: 保存在 TRAPFRAME

2. lazy 

操作系统可以利用页表硬件玩的一个巧妙技巧是延迟分配用户空间堆内存。Xv6 应用程序使用 sbrk() 系统调用来请求堆内存。在我们提供的内核中，sbrk() 分配物理内存并将其映射到进程的虚拟地址空间。然而，有些程序会使用 sbrk() 请求大量内存，但从未使用其中的大部分，例如用于实现大型稀疏数组。为了优化这种情况，高级内核会延迟分配用户内存。也就是说，sbrk() 不会立即分配物理内存，而是仅仅记住哪些地址已被分配。当进程首次尝试使用某一页内存时，CPU 会生成一个页故障，内核通过分配物理内存、清零并映射它来处理这个页故障。在本实验中，你将为 xv6 添加这个延迟分配功能。

>你的第一个任务是从 sbrk(n) 系统调用的实现中删除页分配，该实现位于 sysproc.c 文件中的 sys_sbrk() 函数。sbrk(n) 系统调用将进程的内存大小增加 n 字节，然后返回新分配区域的起始地址（即旧的大小）。你的新 sbrk(n) 只需将进程的大小（myproc()->sz）增加 n 并返回旧的大小。它不应分配内存——因此你应该删除对 growproc() 的调用（但你仍然需要增加进程的大小）！


>修改 trap.c 中的代码，以便在用户空间发生页故障时，通过在故障地址映射新分配的物理内存页面，然后返回用户空间让进程继续执行。你应该在生成 "usertrap(): ..." 消息的 printf 调用之前添加你的代码。如果你的解决方案通过了 usertests，那么它是可以接受的。

一个好的开始方法是修复 trap.c 中的 usertrap()，以便你可以在 shell 中再次运行 echo hi。一旦这个功能正常工作，你会发现一些额外的问题需要解决，以使 usertests 正确运行。以下是一些提示：

你可以通过检查 r_scause() 是否为 13 或 15 来判断故障是否为页故障。
查看 usertrap() 中报告页故障的 printf 调用的参数，以了解如何找到导致页故障的虚拟地址。
从 vm.c 中的 uvmalloc() 函数（sbrk() 通过 growproc() 调用的函数）中借用代码。你需要调用 kalloc() 和 mappages()。
使用 PGROUNDDOWN(va) 将故障虚拟地址向下对齐到页面边界。
uvmunmap() 会触发 panic；修改它，使其在某些页面未映射时不触发 panic。
如果内核崩溃，查看 kernel/kernel.asm 中的 sepc。
使用上面提到的打印函数打印页表的内容。
如果看到错误 “incomplete type proc”，请包含 "proc.h"（以及 "spinlock.h"）。
如果一切顺利，你的延迟分配代码应该能使 echo hi 正常工作。你应该在 shell 中至少遇到一次页故障（从而触发延迟分配），可能还会遇到两次。

## lab 6
1. 线程切换：设计一个用户级线程系统的上下文切换机制，创建线程以及保存/恢复寄存器以在线程之间进行切换
2. 条件变量

## lab 8
1. 设计内存分配器，使得不同 CPU 上的分配和释放可以并行进行，为每个 CPU 维护一个可用内存链表；
2. Buffer cache: 修改缓冲池，给缓冲池增加多个桶，减少进程竞争申请； cmu15445中已经有了，可扩展哈希

## lab9


### sys_link 
sys_link 是一个系统调用，用于在文件系统中创建一个硬链接。硬链接是指向同一 inode 的多个文件名。通过 sys_link，你可以在文件系统中创建一个新的文件名，该文件名指向现有的文件内容。

```cpp
int sys_link(const char *oldpath, const char *newpath);
```
oldpath：现有文件的路径。
newpath：新创建的硬链接的路径。
返回值：成功时返回 0，失败时返回 -1，并设置 errno 表示错误类型。


硬链接只能在同一文件系统内创建，不能跨文件系统。
硬链接不能指向目录，只能指向普通文件。

可以看到硬链接是直接增加inode的计数，所以说当原始文件删除的时候其实 inode的引用减一；使得新的文件名指向与旧文件名相同的 inode。
`if(dp->dev != ip->dev || dirlink(dp, name, ip->inum) < 0)`
检查父目录是否在同一个设备上，并使用 dirlink 函数创建新的目录项。因为每个dir都保持他自己的子目录，所以这里再new dir中添加一个名为 name,并且指向old inode的文件；

而软连接是重新创建一个 inode，这个inode中存的是原始路径；当open这个路径的时候，就去这个inode找个这个路径，然后再通过这个路径找到原始文件。这个过程是递归的；

软连接：文件或者dir都行
硬链接只能：文件

## lab 10 mmp

mmap函数是一种内存映射文件的方法，它可以将一个文件或设备映射到进程的地址空间中，使得进程可以像访问内存一样访问文件或设备。

原理部分就是相当于把文件内容映射成一个内存页，当访问的对应的内存页的时候，触发缺页中断，然后由操作系统走文件系统驱动去读取文件。这个过程中，操作系统和文件系统都全程参与了，并没有越过操作系统或者文件系统。

我们正常读写一个文件，用户代码通常需要先申请一块内存，然后把这块内存下发给内核，然后通过文件系统代码读、写文件。但是这么做有一个性能问题。用户代码的这块内存，通常是不对齐的（不对齐到4K页），那么当文件系统要通过DMA的方式访问磁盘的时候，这块内存通常是不能直接作为DMA buffer下发到硬件驱动里。于是，用户要读一个文件，就需要内核先准备一个对齐的内存下发给驱动（也就是block cache），驱动再下发给硬件，读好数据以后，再从内核的这个内存里复制到用户的内存，需要1-2次内存拷贝（memcpy），如果数据量比较巨大的话，拷贝的开销是很大的。

而mmap就不一样，mmap的内存是由内核初始化的，用户代码只是拿到了一个fd，内核初始化这块内存以后，是可以直接交给文件系统，甚至是块设备驱动使用的，加上Linux的零拷贝（zero copy）技术，可以减少或者避免memcpy动作。同时这块内存又是对用户直接可见的，从内核态切换到用户态的过程中，不需要执行任何拷贝动作。




1. mmap:用于进程之间共享内存，将文件映射到进程的地址空间中，以及作为用户级页错误处理方案的一部分;映射文件描述符，到进程空间；

调用 mmap系统调用：
+ 用户的地址空间的分配中，heap 的范围一直从 stack 到 trapframe。由于进程本身所使用的内存空间是从低地址往高地址生长的（sbrk 调用）。

+ 为了尽量使得 map 的文件使用的地址空间不要和进程所使用的地址空间产生冲突，我们选择将 mmap 映射进来的文件 map 到尽可能高的位置，也就是刚好在 trapframe 下面。

+ 需要一个 vma结构体，来记录mmap映射内存的信息：在用户空间的映射起始地址，大小，要映射的文件描述符，权限，文件偏移；在用户进程描述中添加一个数组用来维护映射多少个内存；

+ 实现 mmap 系统调用。函数原型请参考 man mmap。函数的功能是在进程的 16 个 vma 槽中，找到可用的空槽，并且顺便计算所有 vma 中使用到的最低的虚拟地址（作为新 vma 的结尾地址 vaend，开区间），然后将当前文件映射到该最低地址下面的位置（vastart = vaend - sz）。

+ 由于需要对映射的页实行懒加载，仅在访问到的时候才从磁盘中加载出来；


munmap：






