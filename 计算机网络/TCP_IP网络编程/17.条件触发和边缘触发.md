# 条件触发和边缘触发
## 1. epoll_create1 和 epoll_create
epoll_create1 和 epoll_create 都是用于创建 epoll 实例的系统调用，但它们有一些关键的区别，主要体现在功能和使用上。
  
1. epoll_create
   `int epoll_create(int size);`

epoll_create 是较早版本的 epoll 创建接口。
size 参数指定了 epoll 实例可以监视的文件描述符的数量（这是一个建议值，现代实现中通常被忽略）。
返回一个 epoll 文件描述符，用于后续的 epoll_ctl 操作。
在使用时没有任何额外的标志或选项。

缺点：
  size 参数是过时的，实际不再被使用。
  在现代 Linux 系统中，通常会用 epoll_create1 替代 epoll_create。

2. epoll_create1
  `int epoll_create1(int flags);`

epoll_create1 是在 Linux 2.6.27 版本引入的改进接口。
flags 参数可以设置额外的选项，如 EPOLL_CLOEXEC，用于控制文件描述符的继承行为。
返回一个 epoll 文件描述符，用于后续的 epoll_ctl 操作。
epoll_create1 更加现代化，允许在创建 epoll 实例时直接设置一些标志，简化了文件描述符的管理。

优点：
  支持 EPOLL_CLOEXEC 标志：如果设置了这个标志，创建的文件描述符会自动被设置为 close-on-exec，防止在执行新程序时意外地继承该文件描述符，提高了安全性。
  
  简化了 epoll 的创建过程，无需额外的 fcntl 操作来设置 close-on-exec 标志。

  推荐使用 epoll_create1 来创建 epoll 实例，特别是需要确保文件描述符在 exec 时被正确关闭的场景。

## 2. inet_addr 和  
inet_addr 函数定义在 <arpa/inet.h> 头文件中。这个函数用于将一个点分十进制的 IP 地址字符串（如 "192.168.1.1"）转换为网络字节顺序的 

头文件：<arpa/inet.h>
  这个头文件包含了许多与网络编程相关的函数和常量，例如 inet_addr、inet_ntoa、htons、ntohl 等。

`in_addr_t inet_addr(const char *cp);`
cp：指向以点分十进制形式表示的 IP 地址的字符串。
成功时，返回 IP 地址的网络字节顺序表示（in_addr_t 类型）。如果地址无效，则返回 INADDR_NONE。

```cpp
#include <iostream>
#include <arpa/inet.h>

int main() {
    const char* ip_str = "192.168.1.1";
    in_addr_t addr = inet_addr(ip_str);

    if (addr == INADDR_NONE) {
        std::cerr << "Invalid IP address" << std::endl;
        return 1;
    }

    std::cout << "IP address in network byte order: " << addr << std::endl;

    return 0;
}
```

2. inet_pton

inet_addr 函数在某些情况下可能被认为过时，特别是当你需要处理 IPv6 地址时

inet_pton 是一个用于将 IP 地址从字符串格式转换为网络字节顺序格式的函数。它支持 IPv4 和 IPv6 地址，因此比 inet_addr 更加通用和现代化。inet_pton 是“presentation to network”的缩写，意指将“表示”形式转换为“网络”形式。

```cpp
#include <arpa/inet.h>

int inet_pton(int af, const char *src, void *dst);
```
af：地址族，指定要转换的地址类型。常见值有：
  AF_INET：IPv4 地址。
  AF_INET6：IPv6 地址。
src：指向以字符串形式表示的 IP 地址的源地址（例如 "192.168.1.1" 或 "2001:db8::1"）。
dst：指向接收网络字节顺序 IP 地址的内存区域。对于 IPv4 地址，它是 struct in_addr 类型；对于 IPv6 地址，它是 struct in6_addr 类型。

返回值
  成功：返回 1，表示地址成功转换。
  失败：返回 0，表示提供的地址无效。
  错误：返回 -1，并设置 errno，表示函数调用出错（例如，提供了不支持的地址族）。
```cpp
#include <iostream>
#include <arpa/inet.h>

int main() {
    const char* ipv4_str = "192.168.1.1";
    struct in_addr addr;

    if (inet_pton(AF_INET, ipv4_str, &addr) == 1) {
        std::cout << "IPv4 address in network byte order: " << addr.s_addr << std::endl;
    } else {
        std::cerr << "Invalid IPv4 address" << std::endl;
    }

    return 0;
}
```

在 ET 模式下，它只关心从"没有数据"到"有数据"的转变。
在 ET 模式下，套接字通常需要设置为非阻塞模式，因为读取操作需要反复调用，直到数据全部读取完。阻塞的套接字可能会导致程序挂起。
## 为什么et模式要设置非阻塞模式

在使用 epoll 的边缘触发（ET）模式时，将套接字设置为非阻塞是非常重要的，因为边缘触发模式的工作原理与水平触发（LT）模式有所不同

边缘触发模式 只会在状态发生变化时通知应用程序。例如，当一个套接字从“没有数据”变为“有数据可读”时，epoll 会通知应用程序。在 ET 模式下，一旦事件被触发，你需要一次性处理完所有可读的数据或写入数据，因为 epoll 不会重复通知你。

避免阻塞：
 在 ET 模式下，如果套接字是阻塞的，当你尝试读取数据时，如果没有足够的数据，recv 调用会阻塞，直到有数据可读或者发生错误。由于 ET 模式依赖于处理所有的数据，阻塞会导致应用程序不能立即处理其他事件，也可能导致在一个事件上阻塞，从而影响整个事件循环的效率。

处理所有数据：
 由于 ET 模式只通知一次，必须在一次调用中处理所有的数据。如果套接字是阻塞的，你可能无法在单次读取操作中处理所有数据。非阻塞模式允许你逐步读取数据直到全部处理完成，然后再继续处理其他事件。

##  epoll 的水平触发（LT，Level-Triggered）模式设置非阻塞模式
在 epoll 的水平触发（LT，Level-Triggered）模式下，理论上不需要将套接字设置为非阻塞模式，因为这种模式下 epoll 会不断通知你，直到你把所有数据读取或写入完毕。因此，即便套接字是阻塞的，epoll 也会在数据未完全处理时再次触发事件。然而，在实践中，仍然推荐将套接字设置为非阻塞模式，即使是在 LT 模式下，原因如下：

为什么 LT 模式下推荐非阻塞：
1. 防止程序在 I/O 操作上无限等待
  如果套接字是阻塞的，且你调用 recv 或 send 时，数据尚未完全可用，程序会在这些函数调用上阻塞，导致整个程序无法继续处理其他事件。这种阻塞会降低程序的响应速度，甚至在高并发时导致系统卡顿。
2. 提高响应性能：
  非阻塞模式允许程序及时处理所有事件并返回主循环，即使数据暂时不可用时也不会阻塞。在高并发场景下，这对于提高整体的 I/O 处理效率和降低响应时间非常重要。


 
# 习题
## 1. 利用select函数实现服务器端时，代码层面存在2个缺点是？
1. 循环查看文件描述符  
2. 每次传递监视对象信息
## 2. 无论是select方式还是epoll方式，都需要将监视对象文件描述符信息通过函数调用传递给操作系统。请解释传递该信息的原因
这样做的原因在于操作系统需要了解哪些文件描述符需要被监视，以便在这些文件描述符上发生 I/O 事件时能够及时通知应用程序。

## 3. select方式和epoll方式的最大差异在于监视对象文件描述符传递给操作系统的方式。请说明具体的差异，并解释为何存在这种差异

1. select 方式
在 select 方式中，监视对象的文件描述符是通过每次调用 select() 函数时传递给操作系统的。这意味着：

  每次调用都需要传递所有文件描述符：应用程序在每次调用 select() 时，需要将要监视的所有文件描述符集合（通常是通过 fd_set 数据结构）传递给操作系统。这包括读、写和异常三个不同的文件描述符集合。

  文件描述符的数量有限制：select() 的实现通常对文件描述符数量有上限（通常为 1024 个，受 FD_SETSIZE 限制），这意味着当需要监视大量文件描述符时，select() 的性能会显著下降。

由于每次调用都需要传递所有文件描述符，并且内核在每次调用时需要遍历这些文件描述符进行检查，select 的性能会随着文件描述符数量的增加而下降。

2. epoll 方式

相比之下，epoll 的机制不同，它使用了一个事件注册模型：

  通过 epoll_ctl() 注册文件描述符：在使用 epoll 时，监视的文件描述符通过 epoll_ctl() 函数进行添加、修改或删除。一旦文件描述符注册到 epoll 监视器中，它就会被内核持续监视，直到被显式移除。也就是说，文件描述符的传递是一次性操作。

  无需每次调用都传递文件描述符：当调用 epoll_wait() 时，应用程序不需要再次传递所有的文件描述符，只需等待内核通知哪些文件描述符上发生了事件。内核已经将这些文件描述符存储在高效的数据结构中，等待事件发生。

由于文件描述符的监视状态在内核中持久化存储，epoll 不需要在每次调用时重复传递和遍历所有的文件描述符，这使得它在监视大量文件描述符时具有更高的效率。

3. 差异的原因

这种差异主要是由两者的设计思路和使用场景决定的：

  select 的设计较早，简单易用：select 是一种较早的 I/O 多路复用机制，设计初衷是提供一种简单的接口来同时监视多个文件描述符。然而，它的简单性导致了每次调用都需要重新传递文件描述符集合，适合处理少量文件描述符的场景。

  epoll 设计用于高性能场景：随着网络应用和服务器需要处理大量并发连接，epoll 作为一种改进设计，针对大规模并发连接进行了优化。通过将文件描述符的监视状态保存在内核中，epoll 能够高效处理大量文件描述符而不增加应用程序的开销。

4. 性能对比
   
  select 性能随着文件描述符数量的增加呈线性下降：每次调用都需要遍历所有的文件描述符集合，因此文件描述符越多，性能越差。

  epoll 的性能相对稳定：由于文件描述符在内核中持久化管理，只有发生事件时才通知应用程序，减少了不必要的遍历和数据传递，适合大规模并发处理。

## 4. 虽然epoll是select的改进方案，但select也有自己的优点。在何种情况下使用select方式更合理？
1. 跨平台兼容性要求
select 函数几乎在所有操作系统上都可以使用，而 epoll 是 Linux 特有的 I/O 多路复用机制。

2. 文件描述符数量较少
如果你处理的文件描述符数量较少（例如少于 1024 个），select 的性能可能与 epoll 相当。在这种情况下，使用 select 可以保持代码简单，且不会有显著的性能劣势。

3. 简单性和易用性
select 的 API 更加简单直观，对于小型应用程序或原型开发，select 可能比 epoll 更容易使用。特别是在处理少量的文件描述符时，select 的代码编写相对简单，并且调试更为直接。



## 5. epoll以条件触发或边缘触发方式工作。二者有何区别？从输入缓冲的角度说明这2种方式通知事件的时间点差异

1. 条件触发（水平触发，Level-Triggered，LT）
在 条件触发模式 下，epoll 的行为类似于传统的 select 或 poll。即：

  触发条件：只要文件描述符上的 I/O 事件（如可读、可写）保持存在，epoll 会不断通知应用程序，直到该事件被处理为止。

  多次通知：如果某个文件描述符的输入缓冲区中有数据未被读取，每次调用 epoll_wait() 都会返回该事件，提示应用程序该文件描述符上仍有未处理的数据。

  适合初学者和简单场景：这种模式下，不容易遗漏事件，但可能会导致应用程序在数据未完全读取完毕之前重复收到通知，增加处理开销。

输入缓冲区角度的通知时间点：
当输入缓冲区中有数据可读时，epoll 会立即通知应用程序。只要缓冲区中仍有未读取的数据，epoll 就会不断发出事件通知，即使应用程序尚未处理完该数据。

2. 边缘触发（Edge-Triggered，ET）
  边缘触发模式 是 epoll 的一种高效工作模式，它仅在输入缓冲区从无到有发生变化时通知应用程序。具体表现为：

  触发条件：仅当文件描述符从 不可读状态 变为 可读状态 时，epoll 才会通知应用程序。例如，当输入缓冲区从空变为有数据时，会触发一次通知。

  单次通知：一旦事件被触发并通知了应用程序，epoll 不会再重复通知，除非输入缓冲区再次从无数据变为有数据。应用程序必须确保在收到通知时，将缓冲区中的数据一次性读取完毕，否则可能会错过后续事件。

  高效但易错过事件：这种模式更高效，减少了不必要的通知，但也容易导致遗漏事件。如果应用程序在第一次事件通知时没有完全读取所有数据，后续数据可能会被忽略。

输入缓冲区角度的通知时间点：
当输入缓冲区状态从空变为有数据时，epoll 会立即通知应用程序。这是一个边缘触发的行为，即缓冲区从不可读变为可读的瞬间。之后，即使缓冲区中仍然有未读取的数据，epoll 也不会再通知应用程序，除非缓冲区再次从空变为有数据。

3. 区别总结
条件触发 (LT)：只要缓冲区中有数据存在，每次调用 epoll_wait() 时都会通知应用程序，即使之前已经通知过了。因此，处理事件更加简单，适合需要确保不会遗漏任何事件的场景。

边缘触发 (ET)：只有在缓冲区状态从不可读变为可读时才会通知，之后不会再通知，除非发生新的边缘触发条件。这种模式更高效，但要求应用程序能够一次性处理所有数据，否则可能遗漏事件。

## 6. 采用边缘触发时可以分离数据的接收和处理时间点。说明其原因和优点

1. 原因：事件通知的机制
  边缘触发模式下，epoll 仅在输入缓冲区状态发生变化时（即从无数据到有数据）通知应用程序。因此，应用程序可以在接收到事件通知时立即将数据从内核缓冲区读取到用户空间的缓冲区中，但不必在同一时间点立即处理这些数据。

  分离机制：应用程序在接收到事件通知后，可以只进行数据接收，将数据存储在用户空间的缓冲区中。然后，可以在稍后的某个时间点单独处理这些接收到的数据。这种机制允许将数据接收和数据处理分成两个阶段，从而优化性能和资源管理。

2. 优点
分离数据接收和处理的时间点带来了多方面的优点：

2.1 更高的并发性能 
  非阻塞 I/O：边缘触发模式下，应用程序可以尽可能快地完成数据接收，而不必立即处理数据。这允许应用程序在高并发场景中专注于处理 I/O 操作，减少阻塞等待时间。这样，应用程序可以在更短时间内响应更多的事件，提高整体的并发处理性能。

  解耦接收和处理逻辑：将数据接收和处理解耦后，数据处理逻辑可以在独立的线程或异步任务中进行。这使得 I/O 操作和数据处理可以并行进行，减少了由于数据处理耗时过长而导致的 I/O 事件响应延迟。

2.2 更灵活的资源调度
  批量处理数据：在数据接收阶段，应用程序可以一次性读取大量数据，存储在缓冲区中。随后，在数据处理阶段，可以选择在更合适的时间进行批量处理。这种方式有助于减少频繁的上下文切换和处理开销，特别是在需要对大量数据进行复杂处理时。

  延迟处理：应用程序可以根据当前的系统负载、资源使用情况等因素，选择适合的时间进行数据处理。比如，当系统负载较高时，可以暂时推迟数据处理，优先处理其他更紧急的任务，从而更好地利用系统资源。

2.3 减少重复通知
  避免重复事件通知：在边缘触发模式下，epoll 只在数据到达的边缘触发时通知应用程序，这减少了重复通知的情况。应用程序可以专注于接收数据，不必担心在每次处理前反复接收相同的通知。这样可以减少不必要的 CPU 开销，提升处理效率。

2.4 支持异步编程模型
  支持异步处理：在异步编程模型中，应用程序可以在接收到数据后，迅速返回主循环继续监控其他事件，而数据处理可以通过异步回调或任务队列进行处理。这种方式使得应用程序可以更灵活地安排任务，提升整体系统的响应速度。

## 7. 实现聊天服务器端，使其可以在连接到服务器端的所有客户端之间交换消息。按照条件触发方式和边缘触发方式分别实现epoll服务器端（聊天服务器端的实现中，这2种方式不会产生太大差异）。

能否使用多播实现：
   不合适；多播主要用于在网络中将消息发送给多个特定的接收者（组播组），适用于广播式的通信，比如视频流或实时数据分发

多播的局限性：
  1. 单向通信：多播通常用于从一个发送者向多个接收者广播消息，而不是进行双向通信。每个加入多播组的接收者都可以接收数据，但不能通过多播组向其他接收者发送数据。这意味着多播不适合用于需要双向信息交换的场景，例如聊天服务器。
  2. 消息传递的可靠性：多播使用 UDP 协议，在网络中传输数据时不保证数据到达的可靠性。对于需要保证消息到达的应用，如聊天服务器，UDP 可能不合适，需要额外的机制来处理丢包、顺序等问题。
  3. 组播组限制：多播要求网络设备（如路由器、交换机）支持，并且接收方需要显式加入某个多播组。这个过程对普通用户的透明度不高，不适合需要灵活管理客户端连接的应用。

多播非常适合那些需要将相同的数据发送给多个接收者但无需单独回应的场景，例如：

  实时视频或音频流
  股票行情推送
  在线游戏中的状态同步

信息交换的合适方式
实现聊天服务器或类似的应用，点对点通信或基于 TCP 的解决方案（如 WebSocket 或传统的 TCP 套接字）更适合：

  TCP：使用 TCP 可以保证消息的可靠传输，并且支持双向通信，适合需要保持长连接并交换数据的场景。

  点对点：可以直接通过服务器中转，将消息从一个客户端发送到另一个客户端，实现点对点的信息交换。

```cpp
#include <iostream>
#include <sys/epoll.h>
#include <unistd.h>
#include <fcntl.h>
#include <netinet/in.h>
#include <cstring>
#include <vector>
#include <map>
#define MAX_EVENTS 10
#define BUFFER_SIZE 1024
void error_handling(const char *message);
int setnonblockingmode(int fd);
int main(int argc, char *argv[])
{
    int server_fd, epoll_fd,client_socket;
    struct sockaddr_in server_addr,client_addr;
    char buffer[BUFFER_SIZE];
    int option;
    socklen_t oplen,client_addr_len; 

    if(argc != 2)
    {
        printf("Usage : %s <port>\n", argv[0]);
        exit(1);
    }
    // 创建服务器套接字
    server_fd=socket(PF_INET,SOCK_STREAM,0);
    if(server_fd==-1){
        perror("socket");
        return -1;
    }

    // 设置非阻塞模式
    if(setnonblockingmode(server_fd)==-1){
        close(server_fd);
        return -1;
    }

    //处理time-wait状态
    oplen=sizeof(option);
    option=1;
    if(setsockopt(server_fd,SOL_SOCKET,SO_REUSEADDR,(void*)&option,oplen)){
        perror("setsocket");
        close(server_fd);
        return -1;
    }

    // 设置服务器地址
    memset(&server_addr,0,sizeof(server_addr));
    server_addr.sin_addr.s_addr=htonl(INADDR_ANY);
    server_addr.sin_family=AF_INET;
    server_addr.sin_port=htons(atoi(argv[1]));

    // 绑定套接字
    if((bind(server_fd,(struct sockaddr*)&server_addr,sizeof(server_addr)))==-1)
        error_handling("bind() error");

    // 监听连接  
    if(listen(server_fd,5)==-1)
        error_handling("listen() error");

    // 创建 epoll 实例
    epoll_fd=epoll_create1(0);
    if(epoll_fd==-1){
        perror("epoll_create1");
        close(server_fd);
        return -1;
    }

    // 将服务器套接字加入到 epoll 监视列表中
    struct epoll_event ev;
    ev.events=EPOLLIN;
    ev.data.fd=server_fd;
    if(epoll_ctl(epoll_fd,EPOLL_CTL_ADD,server_fd,&ev)==-1){
        perror("epoll_ctl: server_fd");
        close(server_fd);
        close(epoll_fd);
        return -1;
    }

    //创建事件数组
    struct epoll_event events[MAX_EVENTS];

    //客户端套接字映射
    std::map<int,std::string> clients;

    std::cout << "Server started on port " << atoi(argv[1]) << std::endl;

    while(1)
    {
        int n_fds=epoll_wait(epoll_fd,events,MAX_EVENTS,-1);
        if(n_fds==-1){
            perror("epoll_wait");
            break;
        }
        std::cout <<"n_fds:"<<n_fds<<std::endl;
        for(int i=0;i<n_fds;i++)
        {
            if(events[i].data.fd==server_fd){
                client_addr_len=sizeof(client_addr);
                if((client_socket=accept(server_fd,(struct sockaddr*)&client_addr,&client_addr_len))==-1)
                    printf("accept error \n");
                if(setnonblockingmode(client_socket)==-1){
                    close(client_socket);
                    continue;
                }  
                ev.events=EPOLLIN | EPOLLET;
                ev.data.fd=client_socket;
                if(epoll_ctl(epoll_fd,EPOLL_CTL_ADD,client_socket,&ev)==-1){
                    perror("epoll_ctl: server_fd");
                    close(client_socket);
                    continue;
                }
                clients[client_socket]="";
                std::cout << "New connection, client_fd: " << client_socket << std::endl;
            }else if(events[i].events & EPOLLIN){
                int client_fd=events[i].data.fd;
                while(1){
                    ssize_t count=read(events[i].data.fd,buffer,BUFFER_SIZE);
                    if(count==0){
                        // 客户端断开连接
                        std::cout << "Client disconnected, client_fd: " << client_fd << std::endl;
                        epoll_ctl(epoll_fd,EPOLL_CTL_DEL,client_fd,nullptr);
                        close(client_fd);
                        clients.erase(client_fd);
                        break;
                    }else if(count==-1){
                        if(errno!=EAGAIN){
                            perror("read");
                            close(client_fd);
                            epoll_ctl(epoll_fd,EPOLL_CTL_DEL,client_fd,nullptr);
                            clients.erase(client_fd);
                        }
                        close(client_fd);
                        break;
                    }
                    for(const auto& pair : clients){
                        if(pair.first!=client_fd)
                            write(pair.first,buffer,count);
                    }
                }
            }

        }
    }
    close(server_fd);
    close(epoll_fd);
    return 0;
}
int setnonblockingmode(int fd)
{
    int flag=fcntl(fd,F_GETFL,0);
    if(flag==-1){
        perror("fctnl");
        return -1;
    }
    if(fcntl(fd,F_SETFL,flag | O_NONBLOCK)==-1){
        perror("fcntl");
        return -1;
    }
    return 0;
}
void error_handling(const char *message)
{
    fputs(message,stderr);
    fputc('\n', stderr);
    exit(1);
}
```



